'use client';

import { useState, useEffect, useRef } from 'react';
import { Dialog, DialogContent } from '@/components/ui/dialog';
import { Button } from '@/components/ui/button';
import { Progress } from '@/components/ui/progress';
import { Phone, PhoneOff, Mic, MicOff } from 'lucide-react';
import { useRealtimeVoice } from '@/hooks/useRealtimeVoice';
import { VoiceVisualizer } from './VoiceVisualizer';

interface VoiceCallModalProps {
  isOpen: boolean;
  onClose: () => void;
}

export function VoiceCallModal({ isOpen, onClose }: VoiceCallModalProps) {
  const {
    conversationState,
    conversation,
    currentMessage,
    error,
    startConversation,
    endConversation,
    sendTextMessage,
  } = useRealtimeVoice();

  const [textInput, setTextInput] = useState('');
  const [callDuration, setCallDuration] = useState(0);
  const [isCallActive, setIsCallActive] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [mediaStream, setMediaStream] = useState<MediaStream | null>(null);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const voiceDetectedRef = useRef<boolean>(false);
  const audioBufferRef = useRef<Blob[]>([]);

  // Iniciar conversaci√≥n cuando se abre el modal
  useEffect(() => {
    if (isOpen) {
      setIsCallActive(true);
      setCallDuration(0);
      
      // Iniciar timer
      timerRef.current = setInterval(() => {
        setCallDuration(prev => prev + 1);
      }, 1000);
      
      // Iniciar grabaci√≥n con detecci√≥n de silencio
      console.log('üöÄ MODAL ABIERTO - LLAMANDO A startVoiceRecording');
      startVoiceRecording();
    } else {
      setIsCallActive(false);
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
      stopVoiceRecording();
      endConversation();
    }
    
    return () => {
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
      stopVoiceRecording();
    };
  }, [isOpen, startConversation, endConversation]);

  // Iniciar grabaci√≥n con detecci√≥n de silencio robusta
  const startVoiceRecording = async () => {
    try {
      console.log('üé§ INICIANDO GRABACI√ìN - FUNCI√ìN LLAMADA');
      
      // Configuraci√≥n de audio optimizada (16kHz mono)
      console.log('üé§ SOLICITANDO ACCESO AL MICR√ìFONO...');
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        }
      });
      
      console.log('‚úÖ MICR√ìFONO OBTENIDO:', stream);
      setMediaStream(stream);
      setIsRecording(true);
      
      // Configurar AudioContext para an√°lisis de energ√≠a
      audioContextRef.current = new AudioContext({ sampleRate: 16000 });
      analyserRef.current = audioContextRef.current.createAnalyser();
      const source = audioContextRef.current.createMediaStreamSource(stream);
      source.connect(analyserRef.current);
      
      // Configuraci√≥n del analizador (20ms frames)
      analyserRef.current.fftSize = 512;
      analyserRef.current.smoothingTimeConstant = 0.8;
      const bufferLength = analyserRef.current.frequencyBinCount;
      const dataArray = new Uint8Array(bufferLength);
      
      // Configurar MediaRecorder
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      });
      
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];
      audioBufferRef.current = [];
      voiceDetectedRef.current = false;
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
          audioBufferRef.current.push(event.data);
        }
      };
      
      mediaRecorder.onstop = async () => {
        if (audioChunksRef.current.length > 0 && isCallActive) {
          const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
          await processVoiceAudio(audioBlob);
        }
        
        // Reiniciar grabaci√≥n despu√©s de procesar
        if (isCallActive) {
          setTimeout(() => {
            startVoiceRecording();
          }, 1000);
        }
      };
      
      // Iniciar grabaci√≥n
      console.log('üé§ INICIANDO MEDIARECORDER...');
      mediaRecorder.start(100); // Chunks de 100ms para mejor detecci√≥n
      console.log('‚úÖ MEDIARECORDER INICIADO');
      
      // Funci√≥n de detecci√≥n de silencio robusta
      const detectSilence = () => {
        if (!analyserRef.current || !isCallActive) {
          console.log('‚ùå DETECCI√ìN DETENIDA - analyser o call inactivo');
          return;
        }
        
        console.log('üîç DETECTANDO SILENCIO - FRAME:', Date.now());
        
        analyserRef.current.getByteFrequencyData(dataArray);
        
        // Calcular RMS (Root Mean Square) para detectar energ√≠a
        let sum = 0;
        for (let i = 0; i < bufferLength; i++) {
          sum += dataArray[i] * dataArray[i];
        }
        const rms = Math.sqrt(sum / bufferLength);
        const db = 20 * Math.log10(rms / 255); // Convertir a dB
        
        console.log('üìä RMS:', rms.toFixed(4), 'dB:', db.toFixed(1));
        
        // Umbral m√°s bajo para detectar mejor (-30 dBFS)
        const threshold = -30;
        const isVoiceDetected = db > threshold;
        
        if (isVoiceDetected) {
          console.log('üîä Voz detectada, dB:', db.toFixed(1));
          voiceDetectedRef.current = true;
          
          // Reiniciar timer de silencio (post-roll: 1.5s)
          if (silenceTimerRef.current) {
            clearTimeout(silenceTimerRef.current);
            silenceTimerRef.current = null;
          }
        } else if (voiceDetectedRef.current) {
          // Solo empezar timer de silencio si ya detectamos voz
          if (!silenceTimerRef.current) {
            console.log('üîá Iniciando timer de silencio...');
            silenceTimerRef.current = setTimeout(() => {
              console.log('üîá Silencio confirmado, procesando audio...');
              if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
                mediaRecorderRef.current.stop();
              }
              voiceDetectedRef.current = false;
            }, 1500); // Post-roll: 1.5 segundos
          }
        }
        
        // Continuar detectando
        if (isCallActive) {
          setTimeout(detectSilence, 100); // Cada 100ms en lugar de requestAnimationFrame
        }
      };
      
      // Iniciar detecci√≥n despu√©s de un peque√±o delay
      console.log('üöÄ INICIANDO DETECCI√ìN DE SILENCIO...');
      setTimeout(detectSilence, 500);
      
    } catch (error) {
      console.error('‚ùå Error accediendo al micr√≥fono:', error);
    }
  };

  // Detener grabaci√≥n
  const stopVoiceRecording = () => {
    console.log('üõë Deteniendo grabaci√≥n...');
    setIsRecording(false);
    voiceDetectedRef.current = false;
    
    // Limpiar timers
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
    
    // Detener grabaci√≥n
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
    }
    
    // Cerrar AudioContext
    if (audioContextRef.current) {
      audioContextRef.current.close();
    }
    
    // Detener stream
    if (mediaStream) {
      mediaStream.getTracks().forEach(track => track.stop());
      setMediaStream(null);
    }
  };

  // Procesar audio de voz
  const processVoiceAudio = async (audioBlob: Blob) => {
    try {
      console.log('üé§ Procesando audio de voz...');
      
      const formData = new FormData();
      formData.append('audio', audioBlob, 'recording.webm');

      const response = await fetch('/api/voice/process', {
        method: 'POST',
        body: formData,
      });

      if (response.ok) {
        const result = await response.json();
        console.log('‚úÖ Respuesta del AI:', result);
        
        // Generar audio con ElevenLabs
        console.log('üéµ Generando audio para:', result.response);
        const audioResponse = await fetch('/api/voice/speak', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({ text: result.response }),
        });

        if (audioResponse.ok) {
          const audioBlob = await audioResponse.blob();
          const audioUrl = URL.createObjectURL(audioBlob);
          const audio = new Audio(audioUrl);
          
          audio.onended = () => {
            URL.revokeObjectURL(audioUrl);
            console.log('üéµ Audio terminado, reiniciando grabaci√≥n...');
          };
          
          audio.play();
        }
      }
    } catch (error) {
      console.error('‚ùå Error procesando audio:', error);
    }
  };

  // Cerrar modal y terminar conversaci√≥n
  const handleClose = () => {
    setIsCallActive(false);
    if (timerRef.current) {
      clearInterval(timerRef.current);
    }
    stopVoiceRecording();
    endConversation();
    onClose();
  };

  // Enviar mensaje de texto
  const handleSendText = async (e: React.FormEvent) => {
    e.preventDefault();
    console.log('üöÄ INICIANDO ENV√çO DE MENSAJE:', textInput);
    console.log('üöÄ EVENTO:', e);
    console.log('üöÄ TEXTO INPUT:', textInput);
    
    if (textInput.trim()) {
      try {
        console.log('üì§ Enviando a /api/voice/process-text');
        
        // Usar endpoint HTTP en lugar de WebSocket por ahora
        const response = await fetch('/api/voice/process-text', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({ text: textInput }),
        });

        console.log('üì• Respuesta recibida, status:', response.status);

        if (response.ok) {
          const result = await response.json();
          console.log('‚úÖ Respuesta del AI:', result);
          // No necesitamos setCurrentMessage porque viene del hook
          
          // Generar audio con ElevenLabs
          try {
            console.log('üéµ Generando audio para:', result.response);
            const audioResponse = await fetch('/api/voice/speak', {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
              },
              body: JSON.stringify({ text: result.response }),
            });

            console.log('üéµ Audio response status:', audioResponse.status);
            
            if (audioResponse.ok) {
              const audioBlob = await audioResponse.blob();
              console.log('üéµ Audio blob size:', audioBlob.size);
              const audioUrl = URL.createObjectURL(audioBlob);
              const audio = new Audio(audioUrl);
              
              audio.onloadeddata = () => {
                console.log('üéµ Audio cargado, reproduciendo...');
                audio.play();
              };
              
              audio.onerror = (error) => {
                console.error('üéµ Error reproduciendo audio:', error);
              };
            } else {
              console.error('üéµ Error generando audio:', audioResponse.status, audioResponse.statusText);
            }
          } catch (audioError) {
            console.error('üí• Error en generaci√≥n de audio:', audioError);
          }
        } else {
          console.error('‚ùå Error en process-text:', response.status, response.statusText);
        }
      } catch (error) {
        console.error('üí• Error enviando mensaje:', error);
      }
      setTextInput('');
    } else {
      console.log('‚ö†Ô∏è Texto vac√≠o, no se env√≠a');
    }
  };

  // Formatear tiempo
  const formatTime = (seconds: number) => {
    const minutes = Math.floor(seconds / 60);
    const remainingSeconds = seconds % 60;
    return `${minutes}:${remainingSeconds.toString().padStart(2, '0')}`;
  };

  // Calcular progreso de la conversaci√≥n (simulado)
  const conversationProgress = Math.min((callDuration / 120) * 100, 100);

  return (
    <Dialog open={isOpen} onOpenChange={handleClose}>
      <DialogContent className="sm:max-w-md bg-white border-0 shadow-2xl">
        <div className="flex flex-col items-center space-y-6 p-8">
          {/* T√≠tulo */}
          <div className="text-center">
            <h2 className="text-2xl font-semibold text-gray-900 mb-2">
              Llamada con Recepcionista AI
            </h2>
            <p className="text-gray-600 text-sm">
              Conversaci√≥n directa para agendar tu cita
            </p>
          </div>

          {/* Visualizador de voz */}
          <div className="relative">
            <VoiceVisualizer 
              isActive={conversationState.isActive}
              isListening={conversationState.isListening}
              isSpeaking={conversationState.isSpeaking}
            />
          </div>

          {/* Estado actual */}
          <div className="text-center">
            <p className="text-gray-900 font-medium">
              {conversationState.isSpeaking 
                ? 'AI hablando...' 
                : isRecording 
                ? (voiceDetectedRef.current ? 'Escuchando... (habla)' : 'Esperando que hables...')
                : 'Conectado'
              }
            </p>
            <p className="text-gray-500 text-sm mt-1">
              Duraci√≥n: {formatTime(callDuration)}
            </p>
          </div>

          {/* Progreso de conversaci√≥n */}
          <div className="w-full space-y-2">
            <Progress value={conversationProgress} className="h-2" />
            <p className="text-gray-500 text-xs text-center">
              Progreso de la conversaci√≥n
            </p>
          </div>

          {/* Input de texto alternativo */}
          <form onSubmit={handleSendText} className="w-full space-y-2">
            <input
              type="text"
              value={textInput}
              onChange={(e) => setTextInput(e.target.value)}
              placeholder="O escribe tu mensaje..."
              className="w-full px-3 py-2 border border-gray-300 rounded-lg text-sm focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent"
              disabled={conversationState.isSpeaking}
            />
            <Button
              type="submit"
              size="sm"
              className="w-full bg-blue-600 hover:bg-blue-700 text-white"
              disabled={!textInput.trim() || conversationState.isSpeaking}
            >
              Enviar mensaje
            </Button>
          </form>

          {/* Error */}
          {error && (
            <div className="text-red-500 text-sm text-center bg-red-50 p-3 rounded-lg w-full">
              {error}
            </div>
          )}

          {/* √öltimo mensaje del AI */}
          {currentMessage && (
            <div className="bg-blue-50 border-l-4 border-blue-400 p-4 rounded-lg w-full">
              <p className="text-blue-800 text-sm">{currentMessage}</p>
            </div>
          )}

          {/* Bot√≥n para terminar llamada */}
          <Button
            onClick={handleClose}
            variant="destructive"
            size="lg"
            className="w-full bg-red-600 hover:bg-red-700 text-white"
          >
            <PhoneOff className="w-4 h-4 mr-2" />
            Terminar llamada
          </Button>
        </div>
      </DialogContent>
    </Dialog>
  );
}
